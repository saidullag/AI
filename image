import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

base_model = InceptionV3(weights='imagenet', include_top=False)

image_model = models.Model(inputs=base_model.input, outputs=base_model.layers[-1].output)

def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(299, 299))
    img_array = image.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0)
    img_array = preprocess_input(img_array)
    return img_array

def get_image_features(img_path):
    img_array = preprocess_image(img_path)
    features = image_model.predict(img_array)
    return features

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000, oov_token="<OOV>")

embedding_dim = 256
units = 512
vocab_size = 5001  # Including the <OOV> token
max_length = 49  # Maximum length of the captions


image_input = layers.Input(shape=(2048,))
image_dense = layers.Dense(embedding_dim, activation='relu')(image_input)


caption_input = layers.Input(shape=(max_length,))
caption_embedding = layers.Embedding(vocab_size, embedding_dim, input_length=max_length)(caption_input)
caption_lstm = layers.LSTM(units, return_sequences=True)(caption_embedding)
caption_lstm = layers.Dropout(0.5)(caption_lstm)
caption_lstm = layers.LSTM(units, return_sequences=False)(caption_lstm)
caption_lstm = layers.Dropout(0.5)(caption_lstm)


merged = layers.concatenate([image_dense, caption_lstm])
output = layers.Dense(vocab_size, activation='softmax')(merged)

model = models.Model(inputs=[image_input, caption_input], outputs=output)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()
